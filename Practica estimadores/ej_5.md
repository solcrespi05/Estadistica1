# Ejercicio 5
Considere muestras aleatorias de cada una de las siguientes distribuciones:
i. normal de par√°metros $\mu$ y $\sigma^{2}$  
ii. exponencial de par√°metro ùúÜ;  
iii. Poisson de par√°metro ùúÜ;  
iv. con PDF que depende de un par√°metro ùúÉ ‚àà (0, 1);  

$$f(x; \theta) = \frac{1}{\theta}x^{(\frac{1}{\theta}-1)}I_{[0,1]}(x)
$$  
, donde  

$$I_{[0,1]}(x) = \begin{matrix}
1 & si\: 0\leq x\leq 1,  \\
0 & \textrm{en caso contrario} \\
\end{matrix}.
$$  

v. geom√©trica de par√°metro ùëù;  
vi. gamma de par√°metros ùõº y ùúÜ;  
vii. uniforme U [0, ùúÉ].  
* (a) Hallar en cada caso el estimador de momentos de los par√°metros
* (b) Hallar en cada caso salvo vi el estimador de m√°xima verosimilitud de los par√°metros (salvo vi)
* (c) Para i,ii,iii y vii decir si los estimadores son insesgados o asint√≥ticamente insesgados.
* (d) Calcular el ECM de los estimadores de ùúÉ en vii. Comparando uno con otro, ¬øcu√°l de los dos
estimadores usar√≠as

## Resoluci√≥n 
Voy a poner como t√≠tulo la distribuci√≥n y como items las consignas. Asi esta todos los puntos de la misma distribuci√≥n juntos.  
### Normal
* **a.** En este punto nos piden el estimador de momentos de la normal.  
**M√©todo de momentos**: La idea b√°sica consiste en igualar ciertas caracter√≠sticas
muestrales con las correspondientes caracter√≠sticas poblacionales. Recordemos la
siguiente definici√≥n.   
Entonces para la Normal, la cual tiene 2 par√°metros, llegariamos solo hasta $E(x^{2})$.  
    * Para el primer parametro: $\Rightarrow E(x^{1})=\mu$
    * Para el segundo par√°metro:  

    $$ 
    E(x^{2})= Var(x) + (E(x))^{2}
    $$  

    Siendo $Var(x) = \sigma^{2}$ y $(E(x))^{2}= \mu_{n}^{2}$.  
    Despejamos la varianza asi nos queda sigma despejado.  
    $$Var(x)= E(x^{2}) - (E(x))^{2}$$  
Como es el estimador de metodo de momentos se define a un momento como: $M_1= \frac{1}{n}\sum_{i=1}^{n}x_{i}^{m}$, siendo m el numero de parametro a estimar.  
Entonces si igualamos los estimadores de momentos quedan:  
1. $$E(x^{1}) = M_1 =\mu \Rightarrow \mu=\frac{1}{n}\sum_{i=1}^{n}x_{i}$$
2. 
$$ E(x^{2}) = M_2 =\sigma^{2}+\mu^{2} \Rightarrow \sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2} + (\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2})
$$  

* **b.** Ac√° nos opiden el estimador de m√°xima verosimilitud, este estimador es basicamente buscar la funci√≥n Likelihood a ra√≠z de la funcion acumulad a de la distribuci√≥n(le agregamos $\prod$) y depsues le aplicamos logaritmo, para finalizar la derivamos por cada par√°metro y la igualamos a cero.  

$$
L(\mu,\sigma^{2})= \prod_{i=1}^{n}\frac{1}{\sqrt{2 \pi \sigma^{2}}}exp(-\frac{(x_i -\mu)^{2}}{2\sigma^{2}})
$$  

$$
L(\mu,\sigma^{2}) = (2\pi \sigma^{2})^{-n/2}exp (-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_i -\mu)^{2})
$$  

Aplicamos logaritmo  

$$
l(\mu,\sigma^{2}) = -\frac{n}{2}ln(2\pi) - n\: ln(\theta)-\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}(x_i-\mu)^{2}
$$  

$$
\frac{\partial l(\mu, \sigma^{2} )}{\partial \mu} = \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(x_i - \mu)=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(x_i - \mu) = 0
$$  

$$\Rightarrow \sum_{i=1}^{n}x_i - n\mu =0 \Rightarrow n\mu=\sum_{i=1}^{n}x_i
$$  

$$
\hat{\mu}= \frac{1}{n}\sum_{i=1}^{n}x_i
$$  

