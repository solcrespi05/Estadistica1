# Ejercicio 5
Considere muestras aleatorias de cada una de las siguientes distribuciones:
i. normal de parÃ¡metros $\mu$ y $\sigma^{2}$  
ii. exponencial de parÃ¡metro ðœ†;  
iii. Poisson de parÃ¡metro ðœ†;  
iv. con PDF que depende de un parÃ¡metro ðœƒ âˆˆ (0, 1);  

$$
f(x; \theta) = \frac{1}{\theta}x^{(\frac{1}{\theta}-1)}I_{[0,1]}(x)
$$

, donde  

$$I_{[0,1]}(x) = \begin{matrix}
1 & si\: 0\leq x\leq 1,  \\
0 & \textrm{en caso contrario} \\
\end{matrix}.
$$  

v. geomÃ©trica de parÃ¡metro ð‘;  
vi. gamma de parÃ¡metros ð›¼ y ðœ†;  
vii. uniforme U [0, ðœƒ].  
* (a) Hallar en cada caso el estimador de momentos de los parÃ¡metros
* (b) Hallar en cada caso salvo vi el estimador de mÃ¡xima verosimilitud de los parÃ¡metros (salvo vi)
* (c) Para i,ii,iii y vii decir si los estimadores son insesgados o asintÃ³ticamente insesgados.
* (d) Calcular el ECM de los estimadores de ðœƒ en vii. Comparando uno con otro, Â¿cuÃ¡l de los dos
estimadores usarÃ­as

## ResoluciÃ³n 
Voy a poner como tÃ­tulo la distribuciÃ³n y como items las consignas. Asi esta todos los puntos de la misma distribuciÃ³n juntos.  
### Normal
* **a.** En este punto nos piden el estimador de momentos de la normal.  
**MÃ©todo de momentos**: La idea bÃ¡sica consiste en igualar ciertas caracterÃ­sticas
muestrales con las correspondientes caracterÃ­sticas poblacionales. Recordemos la
siguiente definiciÃ³n.   
Entonces para la Normal, la cual tiene 2 parÃ¡metros, llegariamos solo hasta $E(x^{2})$.  
    * Para el primer parametro: $\Rightarrow E(x^{1})=\mu$
    * Para el segundo parÃ¡metro:  

    $$ 
    E(x^{2})= Var(x) + (E(x))^{2}
    $$  

    Siendo $Var(x) = \sigma^{2}$ y $(E(x))^{2}= \mu_{n}^{2}$.  
    Despejamos la varianza asi nos queda sigma despejado.  
    $$Var(x)= E(x^{2}) - (E(x))^{2}$$  
Como es el estimador de metodo de momentos se define a un momento como: $M_1= \frac{1}{n}\sum_{i=1}^{n}x_{i}^{m}$, siendo m el numero de parametro a estimar.  
Entonces si igualamos los estimadores de momentos quedan:  
1. $$E(x^{1}) = M_1 =\mu \Rightarrow \mu=\frac{1}{n}\sum_{i=1}^{n}x_{i}$$
2. 
$$ E(x^{2}) = M_2 =\sigma^{2}+\mu^{2} \Rightarrow \sigma^{2}=\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2} + (\frac{1}{n}\sum_{i=1}^{n}x_{i}^{2})
$$  

* **b.** AcÃ¡ nos opiden el estimador de mÃ¡xima verosimilitud, este estimador es basicamente buscar la funciÃ³n Likelihood a raÃ­z de la funcion acumulad a de la distribuciÃ³n(le agregamos $\prod$) y depsues le aplicamos logaritmo, para finalizar la derivamos por cada parÃ¡metro y la igualamos a cero.  

$$
L(\mu,\sigma^{2})= \prod_{i=1}^{n}\frac{1}{\sqrt{2 \pi \sigma^{2}}}exp(-\frac{(x_i -\mu)^{2}}{2\sigma^{2}})
$$  

$$
L(\mu,\sigma^{2}) = (2\pi \sigma^{2})^{-n/2}exp (-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_i -\mu)^{2})
$$  

Aplicamos logaritmo  

$$
l(\mu,\sigma^{2}) = -\frac{n}{2}ln(2\pi) - n\: ln(\theta)-\frac{1}{2 \sigma^{2}}\sum_{i=1}^{n}(x_i-\mu)^{2}
$$  

$$
\frac{\partial l(\mu, \sigma^{2} )}{\partial \mu} = \frac{1}{2\sigma^{2}}\sum_{i=1}^{n}2(x_i - \mu)=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(x_i - \mu) = 0
$$  

$$\Rightarrow \sum_{i=1}^{n}x_i - n\mu =0 \Rightarrow n\mu=\sum_{i=1}^{n}x_i
$$  

$$
\hat{\mu}= \frac{1}{n}\sum_{i=1}^{n}x_i
$$  

Ahora buscamos la derivada de $\sigma^{2}$

$$
\frac{\partial l(\mu, \sigma^{2} )}{\partial \sigma^{2}} = -\frac{n}{\sigma} -\sum_{i=1}^{n}(x_i -\mu)^{2}\frac{1}{\sigma^{3}}
$$  

$$
=-\frac{1}{\sigma}+\frac{1}{\sigma^{3}}\sum_{i=1}^{n}(x_i -\mu)^{2} = 0
$$  

$$
n=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(x_i -\mu)^{2} \Rightarrow \hat{\sigma^{2}} = \frac{1}{n}\sum_{i=1}^{n}(x_i - \mu)^{2}
$$  

* c. Decir si los estimadores son insesgados o asintoticamente insesgados.  
Un estimador es insesgado cuanod la esperanza del mismo estimador es igual al parÃ¡metro.  
El estimador asintoticamente insesgado es el que la esperanza del mismo tendiendo n a infinito es igual al parÃ¡metro.  
Es decir:  
    * $E(\hat{\theta}) = \theta$ (insesgado);  
    * n->infinito : $E(\hat{\theta})= \theta$  


$E(\hat{\mu}) = \frac{1}{n} E(\sum_{i=1}^{n}x_i) = \mu$ 
Por lo tanto es insesgado.  

$(n-> \infty);\: E(\hat{\sigma^{2}})= \sigma^{2}$ Este ej esta demostrado en una de las practicas de Ale. Que ahora no tengo, depues lo copio.  





